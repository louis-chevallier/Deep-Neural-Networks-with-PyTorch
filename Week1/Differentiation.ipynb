{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "Differentiation in Pytorch<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import torch\n", "import numpy as np\n", "import pandas as pd\n", "import matplotlib.pylab as plt"]}, {"cell_type": "markdown", "metadata": {}, "source": ["valuation of derivatives in torch"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x = torch.tensor(2, requires_grad=True, dtype=float) #requires_grad tells torch that x value will be used to evaluate functions\n", "y = x**2\n", "#To calculate derivative of y\n", "y.backward() #Backward function Calculates the derivative of y wrt. to x\n", "print(x.grad) "]}, {"cell_type": "markdown", "metadata": {}, "source": ["ets create a new tensor z and evaluate its derivative wrt x"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x = torch.tensor(2, requires_grad=True, dtype=float)\n", "z = x**2 + 2*x + 1\n", "z.backward()\n", "print(x.grad)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["artial Derivatives"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["u = torch.tensor(1, requires_grad=True, dtype=float)\n", "v = torch.tensor(2, requires_grad=True, dtype=float)\n", "f = u*v + u**2\n", "f.backward()\n", "print(u.grad)\n", "print(v.grad)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["############################################################################"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Create a tensor x"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x = torch.tensor(2.0, requires_grad = True)\n", "print(\"The tensor x: \", x)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Create a tensor y according to y = x^2"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y = x ** 2\n", "print(\"The result of y = x^2: \", y)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Take the derivative. Try to print out the derivative at the value x = 2"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y.backward()\n", "print(\"The dervative at x = 2: \", x.grad)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["elow are the attributes of x and y that torch creates. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print('data:',x.data)\n", "print('grad_fn:',x.grad_fn)\n", "print('grad:',x.grad)\n", "print(\"is_leaf:\",x.is_leaf)\n", "print(\"requires_grad:\",x.requires_grad)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print('data:',y.data)\n", "print('grad_fn:',y.grad_fn)\n", "print('grad:',y.grad)\n", "print(\"is_leaf:\",y.is_leaf)\n", "print(\"requires_grad:\",y.requires_grad)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["e can implement our own custom autograd Functions by subclassing torch.autograd.Function <br>\n", "nd implementing the forward and backward passes which operate on Tensors"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class SQ(torch.autograd.Function):"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    @staticmethod\n", "    def forward(ctx,i):\n", "        \"\"\"\n", "        In the forward pass we receive a Tensor containing the input and return\n", "        a Tensor containing the output. ctx is a context object that can be used\n", "        to stash information for backward computation. You can cache arbitrary\n", "        objects for use in the backward pass using the ctx.save_for_backward method.\n", "        \"\"\"\n", "        result=i**2\n", "        ctx.save_for_backward(i)\n", "        return result\n", "    @staticmethod\n", "    def backward(ctx, grad_output):\n", "        \"\"\"\n", "        In the backward pass we receive a Tensor containing the gradient of the loss\n", "        with respect to the output, and we need to compute the gradient of the loss\n", "        with respect to the input.\n", "        \"\"\"\n", "        i, = ctx.saved_tensors\n", "        grad_output = 2*i\n", "        return grad_output"]}, {"cell_type": "markdown", "metadata": {}, "source": ["ets apply the function"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x=torch.tensor(2.0,requires_grad=True )\n", "sq=SQ.apply\n", "y=sq(x)\n", "y\n", "print(y.grad_fn)\n", "y.backward()\n", "x.grad"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Calculate the derivative with multiple values"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x = torch.linspace(-10, 10, 10, requires_grad = True)\n", "Y = x ** 2\n", "y = torch.sum(x ** 2)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Take the derivative with respect to multiple value. Plot out the function and its derivative"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y.backward()\n", "#The method detach()excludes further tracking of operations in the graph, and therefore the subgraph will not record operations\n", "plt.plot(x.detach().numpy(), Y.detach().numpy(), label = 'function')\n", "plt.plot(x.detach().numpy(), x.grad.detach().numpy(), label = 'derivative')\n", "plt.xlabel('x')\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Take the derivative of Relu with respect to multiple value. Plot out the function and its derivative"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x = torch.linspace(-10, 10, 1000, requires_grad = True)\n", "Y = torch.relu(x)\n", "y = Y.sum()\n", "y.backward()\n", "plt.plot(x.detach().numpy(), Y.detach().numpy(), label = 'function')\n", "plt.plot(x.detach().numpy(), x.grad.detach().numpy(), label = 'derivative')\n", "plt.xlabel('x')\n", "plt.legend()\n", "plt.show()\n", "print(y.grad_fn)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}