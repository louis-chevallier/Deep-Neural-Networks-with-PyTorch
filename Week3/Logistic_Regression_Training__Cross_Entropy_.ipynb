{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "Logistic Regression Training Negative Log likelihood (Cross-Entropy)<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import torch\n", "import torch.nn as nn\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "from mpl_toolkits import mplot3d\n", "from torch.utils.data import Dataset, DataLoader"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["torch.manual_seed(1)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["reate class and function for plotting"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class plot_error_surfaces(object):\n", "    \n", "    #Construstor\n", "    def __init__(self, w_range, b_range, X, Y, n_samples = 30, go = True):\n", "        W = np.linspace(-w_range, w_range, n_samples)\n", "        B = np.linspace(-b_range, b_range, n_samples)\n", "        w, b = np.meshgrid(W, B)    \n", "        Z = np.zeros((30, 30))\n", "        count1 = 0\n", "        self.y = Y.numpy()\n", "        self.x = X.numpy()\n", "        for w1, b1 in zip(w, b):\n", "            count2 = 0\n", "            for w2, b2 in zip(w1, b1):\n", "                Z[count1, count2] = np.mean((self.y - (1 / (1 + np.exp(-1*w2 * self.x - b2)))) ** 2)\n", "                count2 += 1   \n", "            count1 += 1\n", "        self.Z = Z\n", "        self.w = w\n", "        self.b = b\n", "        self.W = []\n", "        self.B = []\n", "        self.LOSS = []\n", "        self.n = 0\n", "        if go == True:\n", "            plt.figure()\n", "            plt.figure(figsize=(7.5, 5))\n", "            plt.axes(projection='3d').plot_surface(self.w, self.b, self.Z, rstride=1, cstride=1, cmap='viridis', edgecolor='none')\n", "            plt.title('Loss Surface')\n", "            plt.xlabel('w')\n", "            plt.ylabel('b')\n", "            plt.show()\n", "            plt.figure()\n", "            plt.title('Loss Surface Contour')\n", "            plt.xlabel('w')\n", "            plt.ylabel('b')\n", "            plt.contour(self.w, self.b, self.Z)\n", "            plt.show()\n", "            \n", "     #Setter\n", "    def set_para_loss(self, model, loss):\n", "        self.n = self.n + 1\n", "        self.W.append(list(model.parameters())[0].item())\n", "        self.B.append(list(model.parameters())[1].item())\n", "        self.LOSS.append(loss)\n", "    \n", "    #Plot diagram\n", "    def final_plot(self): \n", "        ax = plt.axes(projection='3d')\n", "        ax.plot_wireframe(self.w, self.b, self.Z)\n", "        ax.scatter(self.W, self.B, self.LOSS, c='r', marker='x', s=200, alpha=1)\n", "        plt.figure()\n", "        plt.contour(self.w, self.b, self.Z)\n", "        plt.scatter(self.W, self.B, c='r', marker='x')\n", "        plt.xlabel('w')\n", "        plt.ylabel('b')\n", "        plt.show()\n", "        \n", "    #Plot diagram\n", "    def plot_ps(self):\n", "        plt.subplot(121)\n", "        plt.ylim\n", "        plt.plot(self.x, self.y, 'ro', label=\"training points\")\n", "        plt.plot(self.x, self.W[-1] * self.x + self.B[-1], label=\"estimated line\")\n", "        plt.plot(self.x, 1 / (1 + np.exp(-1 * (self.W[-1] * self.x + self.B[-1]))), label='sigmoid')\n", "        plt.xlabel('x')\n", "        plt.ylabel('y')\n", "        plt.ylim((-0.1, 2))\n", "        plt.title('Data Space Iteration: ' + str(self.n))\n", "        plt.show()\n", "        plt.subplot(122)\n", "        plt.contour(self.w, self.b, self.Z)\n", "        plt.scatter(self.W, self.B, c='r', marker='x')\n", "        plt.title('Loss Surface Contour Iteration' + str(self.n))\n", "        plt.xlabel('w')\n", "        plt.ylabel('b')\n", "        \n", "#Plot the diagram\n", "def PlotStuff(X, Y, model, epoch, leg=True):\n", "    plt.plot(X.numpy(), model(X).detach().numpy(), label=('epoch ' + str(epoch)))\n", "    plt.plot(X.numpy(), Y.numpy(), 'r')\n", "    if leg == True:\n", "        plt.legend()\n", "    else:\n", "        pass\n", "    \n", "#Create a Data class\n", "class Data(Dataset):\n", "    #Constructor\n", "    def __init__(self):\n", "        self.x = torch.arange(-1, 1, 0.1).view(-1, 1) #Create a column of values between -1 and 1 with 0.1 step\n", "        self.y = torch.zeros(self.x.shape[0], 1) #Create a column of zeros with length of x\n", "        self.y[self.x[:, 0] > 0.2 ] = 1 #y = 1 when x > 0.2\n", "        self.len = self.x.shape[0]\n", "        \n", "    #Getter\n", "    def __getitem__(self, index):\n", "        return self.x[index], self.y[index]\n", "    \n", "    #Length\n", "    def __len__(self):\n", "        return self.len"]}, {"cell_type": "markdown", "metadata": {}, "source": ["reate the dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data_set = Data()\n", "print(data_set.x) #Lets see x\n", "print(data_set.y) #Lets see y"]}, {"cell_type": "markdown", "metadata": {}, "source": ["reate a logistic regression custom class using nn.Module"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class logistic_regression(nn.Module):\n", "    \n", "    #Constructor\n", "    def __init__( self, n_inputs):\n", "        super().__init__()\n", "        self.linear = nn.Linear(n_inputs, 1)\n", "    \n", "    #Prediction\n", "    def forward(self, x):\n", "        yhat = torch.sigmoid(self.linear(x))\n", "        \n", "        return yhat"]}, {"cell_type": "markdown", "metadata": {}, "source": ["nitialize model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model = logistic_regression(1) #1 input model"]}, {"cell_type": "markdown", "metadata": {}, "source": ["anually setting weight and bias"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model.state_dict() ['linear.weight'].data[0] = torch.tensor([[-5]])\n", "model.state_dict() ['linear.bias'].data[0] = torch.tensor([[-10]])\n", "print(\"The parameters: \", model.state_dict())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["reate the plot_error_surfaces object"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["get_surface = plot_error_surfaces(15, 13, data_set[:][0], data_set[:][1], 30)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["reate a DataLoader object"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["trainloader = DataLoader(dataset = data_set, batch_size = 3)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["reate the error metric. Using RMS will cause the manually initialized parameters not to converge<br>\n", "ross Entropy Loss error will be used to solve this problem<br>\n", "e can either manually create a loss function or import from torch"]}, {"cell_type": "markdown", "metadata": {}, "source": ["ets create a cross entropy loss function"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def criterion(yhat, y):\n", "    out = -1 * torch.mean(y * torch.log(yhat) + (1 - y) * torch.log(1 - yhat)) #negative log cross entropy function\n", "    \n", "    return out"]}, {"cell_type": "markdown", "metadata": {}, "source": ["his is the built in cross entropy function in torch<br>\n", "riterion = nn.BCELoss()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["et the learning rate"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["learning_rate = 2"]}, {"cell_type": "markdown", "metadata": {}, "source": ["nitialize the optimizer"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["reate the training loop"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def train_model(epochs):\n", "    for epoch in range(epochs):\n", "        \n", "        for x, y in trainloader:\n", "            yhat = model(x) #predict\n", "            loss = criterion(yhat, y) #calculate loss\n", "            optimizer.zero_grad() #Set gradients to zero\n", "            loss.backward() #Calculate gradients\n", "            optimizer.step() #adjust new parameters\n", "            get_surface.set_para_loss(model, loss.tolist()) #plot\n", "        \n", "        if epoch % 20 == 0:\n", "            get_surface.plot_ps()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["train_model(100)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["rediction"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["yhat = model(data_set.x)\n", "label = yhat > 0.5\n", "print(\"The accuracy: \", torch.mean((label == data_set.y.type(torch.ByteTensor)).type(torch.float))) #Accuracy = 1 or 100%"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}