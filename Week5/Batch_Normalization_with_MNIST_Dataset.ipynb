{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "Batch Normalization with MNIST Dataset<br>\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[1] Imports"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import torch \n", "import torch.nn as nn\n", "import torchvision.transforms as transforms\n", "import torchvision.datasets as dsets\n", "import torch.nn.functional as F\n", "import matplotlib.pylab as plt\n", "import numpy as np\n", "torch.manual_seed(0)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[2] Create Data/Load MNIST Dataset"]}, {"cell_type": "markdown", "metadata": {}, "source": ["load the train dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["train_dataset = dsets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["load the train dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["validation_dataset = dsets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Create Data Loader for both train and validating"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=2000, shuffle=True)\n", "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000, shuffle=False)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[3] Create Neural Network with BatchNorm and without"]}, {"cell_type": "markdown", "metadata": {}, "source": ["NN with BatchNorm"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class NetworkBatchNorm(nn.Module):\n", "    \n", "    # Constructor\n", "    def __init__(self, in_size, H1, H2, out_size):\n", "        super().__init__()\n", "        self.linear1 = nn.Linear(in_size, H1)\n", "        self.linear2 = nn.Linear(H1, H2)\n", "        self.linear3 = nn.Linear(H2, out_size)\n", "        self.bn1 = nn.BatchNorm1d(H1)\n", "        self.bn2 = nn.BatchNorm1d(H2)\n\n", "    # Prediction\n", "    def forward(self, x):\n", "        x = self.linear1(x)\n", "        x = torch.relu(x)\n", "        x = self.bn1(x)\n", "        x = self.linear2(x)\n", "        x = torch.relu(x)\n", "        x = self.bn2(x)\n", "        x = self.linear3(x)\n", "        return x\n", "    \n", "    # Activations to analyze the results\n", "    def activation(self, x):\n", "        out = []\n", "        z1 = self.bn1(self.linear1(x))\n", "        out.append(z1.detach().numpy().reshape(-1))\n", "        a1 = torch.relu(z1)\n", "        out.append(a1.detach().numpy().reshape(-1))\n", "        z2 = self.bn2(self.linear2(a1))\n", "        out.append(z2.detach().numpy().reshape(-1))\n", "        a2 = torch.relu(z2)\n", "        out.append(a2.detach().numpy().reshape(-1))\n", "        return out\n", " \n", "       \n", "# NN without BatchNorm\n", "class Network(nn.Module):\n", "    \n", "    # Constructor\n", "    def __init__(self, in_size, H1, H2, out_size):\n", "        super().__init__()\n", "        self.linear1 = nn.Linear(in_size, H1)\n", "        self.linear2 = nn.Linear(H1, H2)\n", "        self.linear3 = nn.Linear(H2, out_size)\n\n", "    # Prediction\n", "    def forward(self, x):\n", "        x = self.linear1(x)\n", "        x = torch.relu(x)\n", "        x = self.linear2(x)\n", "        x = torch.relu(x)\n", "        x = self.linear3(x)\n", "        return x\n", "    \n", "    # Activations to analyze the results\n", "    def activation(self, x):\n", "        out = []\n", "        z1 = self.linear1(x)\n", "        out.append(z1.detach().numpy().reshape(-1))\n", "        a1 = torch.relu(z1)\n", "        out.append(a1.detach().numpy().reshape(-1))\n", "        z2 = self.linear2(a1)\n", "        out.append(z2.detach().numpy().reshape(-1))\n", "        a2 = torch.relu(z2)\n", "        out.append(a2.detach().numpy().reshape(-1))\n", "        return out"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[4] Create train loop"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Define the function to train model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def train(model, criterion, train_loader, validation_loader, optimizer, epochs=100):\n", "    i = 0\n", "    useful_stuff = {'training_loss':[], 'validation_accuracy':[]}  \n", "    for epoch in range(epochs):\n", "        for i, (x, y) in enumerate(train_loader):\n", "            model.train()\n", "            optimizer.zero_grad()\n", "            z = model(x.view(-1, 28 * 28))\n", "            loss = criterion(z, y)\n", "            loss.backward()\n", "            optimizer.step()\n", "            useful_stuff['training_loss'].append(loss.data.item())\n", "            \n", "        correct = 0\n", "        for x, y in validation_loader:\n", "            model.eval()\n", "            yhat = model(x.view(-1, 28 * 28))\n", "            _, label = torch.max(yhat, 1)\n", "            correct += (label == y).sum().item()\n", "            \n", "        accuracy = 100 * (correct / len(validation_dataset))\n", "        useful_stuff['validation_accuracy'].append(accuracy)\n", "    \n", "    return useful_stuff\n", "        "]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[6] Initialize the model, optimizer, criterion"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Loss function"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["criterion = nn.CrossEntropyLoss()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Initializing both models"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model_batchnorm = NetworkBatchNorm(28*28, 100, 100, 10)\n", "model = Network(28*28, 100, 100, 10)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Optimizer and train"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["optimizer = torch.optim.Adam(model_batchnorm.parameters(), lr=0.1)\n", "training_results_Norm = train(model_batchnorm, criterion, train_loader, validation_loader, optimizer, epochs=5)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n", "training_results = train(model, criterion, train_loader, validation_loader, optimizer, epochs=5)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[5] Model evaluation and Plotting"]}, {"cell_type": "markdown", "metadata": {}, "source": ["set models to evaluation so that batchnorm is put in eval mode."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model.eval()\n", "model_batchnorm.eval()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Plot model activations"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["out = model.activation(validation_dataset[0][0].reshape(-1,28*28))\n", "plt.hist(out[2], label = 'model with no batch normalization' )\n", "plt.xlabel(\"activation \")\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["out_batchnorm = model_batchnorm.activation(validation_dataset[0][0].reshape(-1,28*28))\n", "plt.hist(out_batchnorm[2], label = 'model with normalization')\n", "plt.xlabel(\"activation \")\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Plot the diagram to show the loss"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.plot(training_results['training_loss'], label = 'No Batch Normalization')\n", "plt.plot(training_results_Norm['training_loss'], label = 'Batch Normalization')\n", "plt.ylabel('Cost')\n", "plt.xlabel('iterations ')   \n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Plot the diagram to show the accuracy"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.plot(training_results['validation_accuracy'], label = 'No Batch Normalization')\n", "plt.plot(training_results_Norm['validation_accuracy'], label = 'Batch Normalization')\n", "plt.ylabel('validation accuracy')\n", "plt.xlabel('epochs ')   \n", "plt.legend()\n", "plt.show()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}